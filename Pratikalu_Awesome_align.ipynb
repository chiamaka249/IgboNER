{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pratikalu_Awesome_align.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPcHe9kiwAUztbXy838tVh/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiamaka249/IgboNER/blob/main/Pratikalu_Awesome_align.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AWESOME: Aligning Word Embedding Spaces of Multilingual Encoders**"
      ],
      "metadata": {
        "id": "XVcvKs-Ooh-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "awesome-align is a tool that can extract word alignments from multilingual BERT (mBERT) and allows you to fine-tune mBERT on parallel corpora for better alignment quality (see our paper for more details).\n",
        "\n",
        "This is a simple demo of how awesome-align extracts word alignments from mBERT.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5iJW5-qdoOX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*First, install and import the following packages. (Note that the original awesome-align tool does not require the transformers package.)*"
      ],
      "metadata": {
        "id": "vLAfRHQFpHLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.1.0\n",
        "import torch\n",
        "import transformers\n",
        "import itertools"
      ],
      "metadata": {
        "id": "DoHUAx9UpBHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the multilingual BERT model and its tokenizer."
      ],
      "metadata": {
        "id": "YyLpMgNRpfTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = transformers.BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "id": "wDeurIdApTQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare input sentences\n",
        "Read the English text file and store the sentences in a list ensents = [en_sent1, en_sent2, ..., en_sentn]\n",
        "\n",
        "Do the same for the Igbo text file igsents = [ig_sent1, ig_sent2, ..., ig_sentn]\n",
        "\n",
        "Store each sentence pair tuple in a list file `en_ig_sents = [(en_sent1, ig_sent1), (en_sent2, ig_sent2), ..., (en_sentn, ig_sentn)]"
      ],
      "metadata": {
        "id": "ErUEvPpupqbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Input tokenized source and target sentences.*"
      ],
      "metadata": {
        "id": "I48NHa3TpxKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -c https://raw.githubusercontent.com/Chiamakac/IGBONLP/master/ig_en_mt/benchmark_dataset/test.en\n",
        "!wget -c https://raw.githubusercontent.com/Chiamakac/IGBONLP/master/ig_en_mt/benchmark_dataset/test.ig\n",
        "!wget -c https://raw.githubusercontent.com/Chiamakac/IGBONLP/master/ig_en_mt/benchmark_dataset/train.en\n",
        "!wget -c https://raw.githubusercontent.com/Chiamakac/IGBONLP/master/ig_en_mt/benchmark_dataset/train.ig\n",
        "!wget -c https://raw.githubusercontent.com/Chiamakac/IGBONLP/master/ig_en_mt/benchmark_dataset/val.en\n",
        "!wget -c https://raw.githubusercontent.com/Chiamakac/IGBONLP/master/ig_en_mt/benchmark_dataset/val.ig"
      ],
      "metadata": {
        "id": "YPOj_921pWqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of filenames\n",
        "filenames = ['/content/test.en', '/content/train.en', '/content/val.en']\n",
        "\n",
        "# Open file3 in write mode\n",
        "with open('english.txt', 'w') as outfile:\n",
        "\n",
        "\t# Iterate through list\n",
        "\tfor names in filenames:\n",
        "\n",
        "\t\t# Open each file in read mode\n",
        "\t\twith open(names) as infile:\n",
        "\n",
        "\t\t\t# read the data from file1 and\n",
        "\t\t\t# file2 and write it in file3\n",
        "\t\t\toutfile.write(infile.read())\n",
        "\n",
        "\t\t# Add '\\n' to enter data of file2\n",
        "\t\t# from next line\n",
        "\t\toutfile.write(\"\\n\")"
      ],
      "metadata": {
        "id": "og1tBDC7qAFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of filenames\n",
        "filenames = ['/content/test.ig', '/content/train.ig', '/content/val.ig']\n",
        "\n",
        "# Open file3 in write mode\n",
        "with open('igbo.txt', 'w') as outfile:\n",
        "\n",
        "\t# Iterate through list\n",
        "\tfor names in filenames:\n",
        "\n",
        "\t\t# Open each file in read mode\n",
        "\t\twith open(names) as infile:\n",
        "\n",
        "\t\t\t# read the data from file1 and\n",
        "\t\t\t# file2 and write it in file3\n",
        "\t\t\toutfile.write(infile.read())\n",
        "\n",
        "\t\t# Add '\\n' to enter data of file2\n",
        "\t\t# from next line"
      ],
      "metadata": {
        "id": "vmPIG_3pqFgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# opening the file in read mode\n",
        "en_file = open(\"/content/english.txt\", \"r\")\n",
        "\n",
        "# reading the file\n",
        "ensents = en_file.read()\n",
        "\n",
        "# replacing end splitting the text\n",
        "# when newline ('\\n') is seen.\n",
        "ensents = ensents.split(\"\\n\")\n",
        "# print(ensents)\n",
        "en_file.close()\n"
      ],
      "metadata": {
        "id": "yoS87q_tqJmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# program to convert a list into a tuple\n",
        "def convert(ensents):\n",
        "\treturn tuple(ensents)\n",
        "\n",
        "# Driver function\n",
        "ensents= ensents\n",
        "# print(convert(ensents))\n"
      ],
      "metadata": {
        "id": "i6j61uF_qN33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# opening the file in read mode\n",
        "ig_file = open(\"/content/igbo.txt\", \"r\")\n",
        "\n",
        "# reading the file\n",
        "igsents = ig_file.read()\n",
        "\n",
        "# replacing end splitting the text\n",
        "# when newline ('\\n') is seen.\n",
        "igsents = igsents.split(\"\\n\")\n",
        "# print(igsents)\n",
        "ig_file.close()"
      ],
      "metadata": {
        "id": "Usmpgs5oqSw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# program to convert a list into a tuple\n",
        "def convert(igsents):\n",
        "\treturn tuple(igsents)\n",
        "\n",
        "# Driver function\n",
        "igsents= igsents\n",
        "# print(convert(igsents))"
      ],
      "metadata": {
        "id": "DmMOwVm4qXyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python zip() function can be used to map the lists altogether \n",
        "# to create a list of tuples using the command:list(zip(list))\n",
        "lst1 = ensents\n",
        "lst2 = igsents\n",
        "en_ig_sents = list(zip(lst1,lst2))\n",
        "print(en_ig_sents)"
      ],
      "metadata": {
        "id": "f8503kY3qbri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for src, tgt in en_ig_sents:\n",
        "  perform the alignment"
      ],
      "metadata": {
        "id": "MMqF2jqNqhBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the model and print the resulting alignments."
      ],
      "metadata": {
        "id": "rfOjxPKeqnZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-processing\n",
        "sent_src, sent_tgt = src.strip().split(), tgt.strip().split()\n",
        "token_src, token_tgt = [tokenizer.tokenize(word) for word in sent_src], [tokenizer.tokenize(word) for word in sent_tgt]\n",
        "wid_src, wid_tgt = [tokenizer.convert_tokens_to_ids(x) for x in token_src], [tokenizer.convert_tokens_to_ids(x) for x in token_tgt]\n",
        "ids_src, ids_tgt = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids'], tokenizer.prepare_for_model(list(itertools.chain(*wid_tgt)), return_tensors='pt', truncation=True, model_max_length=tokenizer.model_max_length)['input_ids']\n",
        "sub2word_map_src = []\n",
        "for i, word_list in enumerate(token_src):\n",
        "  sub2word_map_src += [i for x in word_list]\n",
        "sub2word_map_tgt = []\n",
        "for i, word_list in enumerate(token_tgt):\n",
        "  sub2word_map_tgt += [i for x in word_list]\n",
        "\n",
        "# alignment\n",
        "align_layer = 8\n",
        "threshold = 1e-3\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "  out_tgt = model(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "\n",
        "  dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))\n",
        "\n",
        "  softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)\n",
        "  softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)\n",
        "\n",
        "  softmax_inter = (softmax_srctgt > threshold)*(softmax_tgtsrc > threshold)\n",
        "\n",
        "align_subwords = torch.nonzero(softmax_inter, as_tuple=False)\n",
        "align_words = set()\n",
        "for i, j in align_subwords:\n",
        "  align_words.add( (sub2word_map_src[i], sub2word_map_tgt[j]) )\n",
        "\n",
        "# printing\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "\n",
        "for i, j in sorted(align_words):\n",
        "  print(f'{color.BOLD}{color.BLUE}{sent_src[i]}{color.END}==={color.BOLD}{color.RED}{sent_tgt[j]}{color.END}')"
      ],
      "metadata": {
        "id": "1E1es1neqfTH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}